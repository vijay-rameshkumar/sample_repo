{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_ranking as tfr\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2801 entries, 23 to 33304\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   supplier_id  2801 non-null   object \n",
      " 1   subjects     2801 non-null   object \n",
      " 2   score        2801 non-null   float32\n",
      "dtypes: float32(1), object(2)\n",
      "memory usage: 76.6+ KB\n"
     ]
    }
   ],
   "source": [
    "positive_samples = pd.read_csv('adj_matrx_v2/01_adjacency_continent_lang_study_subject_weighted_suppliers_average_profit.csv')\n",
    "suppliers = positive_samples.suppliers_info.values.tolist()\n",
    "positive_samples = positive_samples.set_index('suppliers_info')\n",
    "positive_samples = positive_samples.sort_index()\n",
    "positive_samples = positive_samples.stack().reset_index()\n",
    "\n",
    "positive_samples.columns = ['supplier_id', 'subjects', 'score']\n",
    "\n",
    "positive_samples['supplier_id'] = positive_samples['supplier_id'].astype('str')\n",
    "positive_samples['score'] = positive_samples['score'].astype('float32')\n",
    "\n",
    "positive_samples = positive_samples[positive_samples.score != 0.0]\n",
    "\n",
    "positive_samples.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SURPRISE Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import SVD, SVDpp, NormalPredictor, KNNBasic, BaselineOnly, CoClustering\n",
    "from surprise import SlopeOne, KNNBaseline, KNNWithMeans, KNNWithZScore\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "reader = Reader(rating_scale=(0, 1))\n",
    "\n",
    "data = Dataset.load_from_df(positive_samples[['supplier_id', 'subjects', 'score']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n"
     ]
    }
   ],
   "source": [
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for algorithm in [SVD(), SVDpp(), SlopeOne(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    \n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNNBaseline</th>\n",
       "      <td>13.287508</td>\n",
       "      <td>0.011642</td>\n",
       "      <td>0.069972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaselineOnly</th>\n",
       "      <td>13.291603</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>13.297514</td>\n",
       "      <td>0.070772</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBasic</th>\n",
       "      <td>13.300853</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>0.044332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SlopeOne</th>\n",
       "      <td>13.312480</td>\n",
       "      <td>0.003656</td>\n",
       "      <td>0.005011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVDpp</th>\n",
       "      <td>13.328593</td>\n",
       "      <td>0.228628</td>\n",
       "      <td>0.010751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithMeans</th>\n",
       "      <td>13.351541</td>\n",
       "      <td>0.015396</td>\n",
       "      <td>0.065041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoClustering</th>\n",
       "      <td>13.366697</td>\n",
       "      <td>0.074164</td>\n",
       "      <td>0.003345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithZScore</th>\n",
       "      <td>13.373244</td>\n",
       "      <td>0.023390</td>\n",
       "      <td>0.069141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NormalPredictor</th>\n",
       "      <td>13.441000</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.003665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test_rmse  fit_time  test_time\n",
       "Algorithm                                      \n",
       "KNNBaseline      13.287508  0.011642   0.069972\n",
       "BaselineOnly     13.291603  0.001000   0.002005\n",
       "SVD              13.297514  0.070772   0.004000\n",
       "KNNBasic         13.300853  0.009185   0.044332\n",
       "SlopeOne         13.312480  0.003656   0.005011\n",
       "SVDpp            13.328593  0.228628   0.010751\n",
       "KNNWithMeans     13.351541  0.015396   0.065041\n",
       "CoClustering     13.366697  0.074164   0.003345\n",
       "KNNWithZScore    13.373244  0.023390   0.069141\n",
       "NormalPredictor  13.441000  0.000998   0.003665"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples.columns = ['suppliers__ref', 'projects__study_types_subject_ids', 'positive_score']\n",
    "\n",
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(positive_samples['suppliers__ref'].values, tf.string),\n",
    "            tf.cast(positive_samples['projects__study_types_subject_ids'].values, tf.string),\n",
    "            tf.cast(positive_samples['positive_score'].values, tf.float32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "ratings = training_dataset.map(lambda x,y,z: {\n",
    "    \"movie_title\": y,\n",
    "    \"user_id\": x,\n",
    "    \"user_rating\": z,\n",
    "})\n",
    "movies = ratings.map(lambda x:x['movie_title'])\n",
    "\n",
    "unique_movie_titles = positive_samples['projects__study_types_subject_ids'].unique()\n",
    "unique_user_ids = positive_samples['suppliers__ref'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Split between train and tests sets, as before.\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(100_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "# We sample 50 lists for each user for the training data. For each list we\n",
    "# sample 5 movies from the movies the user rated.\n",
    "train = tfrs.examples.movielens.sample_listwise(\n",
    "    train,\n",
    "    num_list_per_user=50,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test = tfrs.examples.movielens.sample_listwise(\n",
    "    test,\n",
    "    num_list_per_user=1,\n",
    "    num_examples_per_list=5,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
      "array([b'b2b@technology', b'consumer_study@print_social_media',\n",
      "       b'consumer_study@other', b'consumer_study@entertainment',\n",
      "       b'b2b@security'], dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'south-america@por@271'>,\n",
      " 'user_rating': <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([0.18429855, 0.900848  , 0.49826503, 1.1475626 , 0.2804881 ],\n",
      "      dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "for example in train.take(1):\n",
    "  pprint.pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, loss):\n",
    "    super().__init__()\n",
    "    embedding_dimension = 32\n",
    "\n",
    "    # Compute embeddings for users.\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=tf.convert_to_tensor(unique_user_ids)),\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for movies.\n",
    "    self.movie_embeddings = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=tf.convert_to_tensor(unique_movie_titles)),\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 2, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.score_model = tf.keras.Sequential([\n",
    "      # Learn multiple dense layers.\n",
    "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "      # Make rating predictions in the final layer.\n",
    "      tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    self.task = tfrs.tasks.Ranking(\n",
    "      loss=loss,\n",
    "      metrics=[\n",
    "        tfr.keras.metrics.NDCGMetric(name=\"ndcg_metric\"),\n",
    "        tf.keras.metrics.RootMeanSquaredError()\n",
    "      ]\n",
    "    )\n",
    "\n",
    "  def call(self, features):\n",
    "    # We first convert the id features into embeddings.\n",
    "    # User embeddings are a [batch_size, embedding_dim] tensor.\n",
    "    user_embeddings = self.user_embeddings(features[\"user_id\"])\n",
    "\n",
    "    # Movie embeddings are a [batch_size, num_movies_in_list, embedding_dim]\n",
    "    # tensor.\n",
    "    movie_embeddings = self.movie_embeddings(features[\"movie_title\"])\n",
    "\n",
    "    # We want to concatenate user embeddings with movie emebeddings to pass\n",
    "    # them into the ranking model. To do so, we need to reshape the user\n",
    "    # embeddings to match the shape of movie embeddings.\n",
    "    list_length = features[\"movie_title\"].shape[1]\n",
    "    user_embedding_repeated = tf.repeat(\n",
    "        tf.expand_dims(user_embeddings, 1), [list_length], axis=1)\n",
    "\n",
    "    # Once reshaped, we concatenate and pass into the dense layers to generate\n",
    "    # predictions.\n",
    "    concatenated_embeddings = tf.concat(\n",
    "        [user_embedding_repeated, movie_embeddings], 2)\n",
    "\n",
    "    return self.score_model(concatenated_embeddings)\n",
    "\n",
    "  def compute_loss(self, features, training=False):\n",
    "    labels = features.pop(\"user_rating\")\n",
    "\n",
    "    scores = self(features)\n",
    "\n",
    "    return self.task(\n",
    "        labels=labels,\n",
    "        predictions=tf.squeeze(scores, axis=-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(512).cache()\n",
    "cached_test = test.batch(64).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "18/18 [==============================] - 1s 8ms/step - ndcg_metric: 0.7569 - root_mean_squared_error: 10.4586 - loss: 104.1342 - regularization_loss: 0.0000e+00 - total_loss: 104.1342\n",
      "Epoch 2/3\n",
      "18/18 [==============================] - 0s 7ms/step - ndcg_metric: 0.8105 - root_mean_squared_error: 7.7591 - loss: 58.9323 - regularization_loss: 0.0000e+00 - total_loss: 58.9323\n",
      "Epoch 3/3\n",
      "18/18 [==============================] - 0s 7ms/step - ndcg_metric: 0.8192 - root_mean_squared_error: 7.3089 - loss: 52.5069 - regularization_loss: 0.0000e+00 - total_loss: 52.5069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fee58a4820>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_model = RankingModel(tf.keras.losses.MeanSquaredError())\n",
    "mse_model.compile(optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='root_mean_squared_error', patience=3, restore_best_weights=True, verbose=0)\n",
    "\n",
    "mse_model.fit(cached_train, epochs=epochs, verbose=True, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise hinge loss model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "18/18 [==============================] - 1s 7ms/step - ndcg_metric: 0.7580 - root_mean_squared_error: 12.1663 - loss: 1.7026 - regularization_loss: 0.0000e+00 - total_loss: 1.7026\n",
      "Epoch 2/3\n",
      "18/18 [==============================] - 0s 8ms/step - ndcg_metric: 0.7595 - root_mean_squared_error: 12.2003 - loss: 1.6391 - regularization_loss: 0.0000e+00 - total_loss: 1.6391\n",
      "Epoch 3/3\n",
      "18/18 [==============================] - 0s 8ms/step - ndcg_metric: 0.7626 - root_mean_squared_error: 12.2439 - loss: 1.5386 - regularization_loss: 0.0000e+00 - total_loss: 1.5386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fed7dd5fa0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hinge_model = RankingModel(tfr.keras.losses.PairwiseHingeLoss())\n",
    "hinge_model.compile(optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='root_mean_squared_error', patience=3, restore_best_weights=True, verbose=0)\n",
    "\n",
    "hinge_model.fit(cached_train, epochs=epochs, verbose=True, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ListWise Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "18/18 [==============================] - 1s 7ms/step - ndcg_metric: 0.8349 - root_mean_squared_error: 11.5292 - loss: 3.9561 - regularization_loss: 0.0000e+00 - total_loss: 3.9561\n",
      "Epoch 2/3\n",
      "18/18 [==============================] - 0s 7ms/step - ndcg_metric: 0.9508 - root_mean_squared_error: 10.1841 - loss: 2.4693 - regularization_loss: 0.0000e+00 - total_loss: 2.4693\n",
      "Epoch 3/3\n",
      "18/18 [==============================] - 0s 7ms/step - ndcg_metric: 0.9779 - root_mean_squared_error: 9.9314 - loss: 1.8389 - regularization_loss: 0.0000e+00 - total_loss: 1.8389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fee59a9ee0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listwise_model = RankingModel(tfr.keras.losses.ListMLELoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "\n",
    "listwise_model.fit(cached_train, epochs=epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listwise_model = RankingModel(tfr.keras.losses.ApproxMRRLoss())\n",
    "listwise_model.compile(optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "\n",
    "listwise_model.fit(cached_train, epochs=epochs, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d437c2bd0d26a9909614bcc5b04828d6277ae5f269770a7161a0a726ff076161"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
