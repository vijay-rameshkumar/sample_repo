{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (55,56,57,58,60,61,62,64,65,66,67,68,69,70,71) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163056\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supplier_sample_pulls__created_at</th>\n",
       "      <th>supplier_sample_pulls__sampling_method</th>\n",
       "      <th>sample_pulls__target_groups</th>\n",
       "      <th>sample_pulls__country</th>\n",
       "      <th>sample_pulls__language</th>\n",
       "      <th>sample_pulls__cost_per_interview</th>\n",
       "      <th>sample_pulls__incident_rate</th>\n",
       "      <th>sample_pulls__length_of_interview</th>\n",
       "      <th>sample_pulls__completes_needed</th>\n",
       "      <th>sample_pulls__start_date</th>\n",
       "      <th>...</th>\n",
       "      <th>qualifications_ZIP</th>\n",
       "      <th>qualifications_GENDER</th>\n",
       "      <th>qualifications_DMA</th>\n",
       "      <th>qualifications_PROVINCE_OR_TERRITORY_OF_CANADA</th>\n",
       "      <th>qualifications_STANDARD_COMPANY_REVENUE</th>\n",
       "      <th>qualifications_HISPANIC</th>\n",
       "      <th>qualifications_STANDARD_COMPANY_DEPARTMENT</th>\n",
       "      <th>qualifications_STANDARD_DIAGNOSED_AILMENTS_I</th>\n",
       "      <th>qualifications_REGION</th>\n",
       "      <th>qualifications_ETHNICITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-21T22:56:13.751Z</td>\n",
       "      <td>manual</td>\n",
       "      <td>[{\"name\": \"Welding\", \"quota\": 150, \"qualificat...</td>\n",
       "      <td>usa</td>\n",
       "      <td>eng</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-21T22:56:13.751Z</td>\n",
       "      <td>manual</td>\n",
       "      <td>[{\"name\": \"Welding\", \"quota\": 150, \"qualificat...</td>\n",
       "      <td>usa</td>\n",
       "      <td>eng</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-21T22:56:13.751Z</td>\n",
       "      <td>manual</td>\n",
       "      <td>[{\"name\": \"Welding\", \"quota\": 150, \"qualificat...</td>\n",
       "      <td>usa</td>\n",
       "      <td>eng</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-21T22:56:13.751Z</td>\n",
       "      <td>manual</td>\n",
       "      <td>[{\"name\": \"Welding\", \"quota\": 150, \"qualificat...</td>\n",
       "      <td>usa</td>\n",
       "      <td>eng</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-21T22:56:13.751Z</td>\n",
       "      <td>manual</td>\n",
       "      <td>[{\"name\": \"Welding\", \"quota\": 150, \"qualificat...</td>\n",
       "      <td>usa</td>\n",
       "      <td>eng</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>2022-04-21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  supplier_sample_pulls__created_at supplier_sample_pulls__sampling_method  \\\n",
       "0          2022-04-21T22:56:13.751Z                                 manual   \n",
       "1          2022-04-21T22:56:13.751Z                                 manual   \n",
       "2          2022-04-21T22:56:13.751Z                                 manual   \n",
       "3          2022-04-21T22:56:13.751Z                                 manual   \n",
       "4          2022-04-21T22:56:13.751Z                                 manual   \n",
       "\n",
       "                         sample_pulls__target_groups sample_pulls__country  \\\n",
       "0  [{\"name\": \"Welding\", \"quota\": 150, \"qualificat...                   usa   \n",
       "1  [{\"name\": \"Welding\", \"quota\": 150, \"qualificat...                   usa   \n",
       "2  [{\"name\": \"Welding\", \"quota\": 150, \"qualificat...                   usa   \n",
       "3  [{\"name\": \"Welding\", \"quota\": 150, \"qualificat...                   usa   \n",
       "4  [{\"name\": \"Welding\", \"quota\": 150, \"qualificat...                   usa   \n",
       "\n",
       "  sample_pulls__language  sample_pulls__cost_per_interview  \\\n",
       "0                    eng                              17.5   \n",
       "1                    eng                              17.5   \n",
       "2                    eng                              17.5   \n",
       "3                    eng                              17.5   \n",
       "4                    eng                              17.5   \n",
       "\n",
       "   sample_pulls__incident_rate  sample_pulls__length_of_interview  \\\n",
       "0                          0.3                                  8   \n",
       "1                          0.3                                  8   \n",
       "2                          0.3                                  8   \n",
       "3                          0.3                                  8   \n",
       "4                          0.3                                  8   \n",
       "\n",
       "   sample_pulls__completes_needed sample_pulls__start_date  ...  \\\n",
       "0                             150               2022-04-21  ...   \n",
       "1                             150               2022-04-21  ...   \n",
       "2                             150               2022-04-21  ...   \n",
       "3                             150               2022-04-21  ...   \n",
       "4                             150               2022-04-21  ...   \n",
       "\n",
       "  qualifications_ZIP qualifications_GENDER qualifications_DMA  \\\n",
       "0                NaN                   NaN                NaN   \n",
       "1                NaN                   NaN                NaN   \n",
       "2                NaN                   NaN                NaN   \n",
       "3                NaN                   NaN                NaN   \n",
       "4                NaN                   NaN                NaN   \n",
       "\n",
       "  qualifications_PROVINCE_OR_TERRITORY_OF_CANADA  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "\n",
       "   qualifications_STANDARD_COMPANY_REVENUE  qualifications_HISPANIC  \\\n",
       "0                                      NaN                      NaN   \n",
       "1                                      NaN                      NaN   \n",
       "2                                      NaN                      NaN   \n",
       "3                                      NaN                      NaN   \n",
       "4                                      NaN                      NaN   \n",
       "\n",
       "   qualifications_STANDARD_COMPANY_DEPARTMENT  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "\n",
       "   qualifications_STANDARD_DIAGNOSED_AILMENTS_I qualifications_REGION  \\\n",
       "0                                           NaN                   NaN   \n",
       "1                                           NaN                   NaN   \n",
       "2                                           NaN                   NaN   \n",
       "3                                           NaN                   NaN   \n",
       "4                                           NaN                   NaN   \n",
       "\n",
       "  qualifications_ETHNICITY  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATAPATH = 'data/subjectwise_transformed_big_dfs/dfs/subject_other.csv' \n",
    "\n",
    "df = pd.read_csv(DATAPATH)\n",
    "#df = df[~df.overview.isna()]\n",
    "df.rename(columns={'projects__target_groups_qualifications_combine':'sentence'}, inplace=True)\n",
    "print(len(df))\n",
    "#df = df.iloc[:20000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 163056 entries, 0 to 163055\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   rl              163056 non-null  int64 \n",
      " 1   projects__name  163056 non-null  object\n",
      " 2   sentence        163056 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df[['rl','projects__name','sentence']]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['project_supplier_id'] = df['projects__name'].map(str) + '-' + df['rl'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rl projects__name                                           sentence  \\\n",
      "0  528  Welding Study  {'STANDARD_B2B_DECISION_MAKER': ['Other', 'Off...   \n",
      "1  528  Welding Study  {'STANDARD_B2B_DECISION_MAKER': ['Other', 'Off...   \n",
      "2  528  Welding Study  {'STANDARD_B2B_DECISION_MAKER': ['Other', 'Off...   \n",
      "3  528  Welding Study  {'STANDARD_B2B_DECISION_MAKER': ['Other', 'Off...   \n",
      "4  528  Welding Study  {'STANDARD_B2B_DECISION_MAKER': ['Other', 'Off...   \n",
      "\n",
      "  project_supplier_id  \n",
      "0   Welding Study-528  \n",
      "1   Welding Study-528  \n",
      "2   Welding Study-528  \n",
      "3   Welding Study-528  \n",
      "4   Welding Study-528  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 163056 entries, 0 to 163055\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   rl                   163056 non-null  int64 \n",
      " 1   projects__name       163056 non-null  object\n",
      " 2   sentence             163056 non-null  object\n",
      " 3   project_supplier_id  163056 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 5.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "print(df.projects__name.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welding Study' 'Product DMs/Business Elites' 'USA Accounting Director+'\n",
      " 'OR 14135' 'NEA MTA Study- Age 18+, Voters in Massachusetts '\n",
      " 'Suffolk Community College | LONR3216 | Long Island'\n",
      " 'Non African American male 35-54 in Clark county NV'\n",
      " 'middle eastern in USA_YGO-66939  NTHW0011 '\n",
      " ' Tulsa Oilers | TULT3253 | Tulsa (Panel 1)'\n",
      " ' Disney+ Subscribers (2022.D1.040522)' 'Women age 50+'\n",
      " 'Healthcare/Skilled Trade 22-0750' 'YGP - 21153 WAFFLE YGO - 66669'\n",
      " 'US; Desktop only'\n",
      " 'independent contractors, consultants, temp workers, part time employees'\n",
      " 'Brazil & Indonesia - E-Commerce Merchants'\n",
      " 'Bid 14233 College admins 4.15.22'\n",
      " 'OR 14220-  Restaurant Owners and Decision Makers' 'AZ354_PIRR_0422'\n",
      " 'information workers - 147268' 'OR bid 14077 auto' 'Affeld - Ho'\n",
      " 'Minority owned or operated businesses'\n",
      " 'Ages 18+ within Comcast footprint_GSL100893KM: Comcast: Ed Helms Xfinity AdPi Express'\n",
      " '(QM0417) | TurboTax DIFM Core Benefit Research 2022'\n",
      " 'F&B Manaufacturing' 'Corwin - Anna'\n",
      " 'Coffee Creamer Behavior Study - Philadelphia & Tampa/Orlando (OPP138799)'\n",
      " 'Auto repair shops/Dealerships in USA_OEM & Aftermarket Car Parts Study | IS# 4346'\n",
      " 'Gen-pop AZ278_WCPA_0422'\n",
      " 'TRACKER: Bank of America MONTHLY Covid Tracker ' 'Genpop in US and CA'\n",
      " 'AZ201_SWLM_0422' 'Age 18-54 in US'\n",
      " 'Ad-Hoc Battery purchasers B2B - Quest (371-0300)' 'Nat Equis survey'\n",
      " 'SBOs: security feature study'\n",
      " 'Ages 18+ within Comcast footprint_[GSL100893KI]_Comcast Copy Test: AT&T Golf AdPi Express'\n",
      " ' WHBQ May 2022 Sweeps Topics Study | Memphis, Tennessee DMA | Panel ID: MEMT3269'\n",
      " 'SWM211218 Community Sample Refresh'\n",
      " 'Bid 13771 DTE - Marking Sizing and Competitive Intelligence 2.24.22'\n",
      " 'OR 14156- Age 50+, University Faculty' '18-74 Florida Market'\n",
      " 'Birth Control Tracker - April Wave' 'Business Sentiment Tracker Q2 2022'\n",
      " 'Sleep Sensing Survey' 'FDMs in CA' 'USA Ages 18-44'\n",
      " '18-50 in given zips' 'Concrete/Roofers - 2022 Q2 Wave'\n",
      " 'USA landscapers ' ' LAC Survey' 'FTEs/PTEs in given DMAs'\n",
      " 'US_Questmindshare_SCPO 246185'\n",
      " \"OR 14105- DM's in Business with EE size 1-99 in New York city\"\n",
      " 'CX Pros & cons' 'College Students_Ages 18+'\n",
      " 'Architects/General Contractors/Builders/Interior designers_h22034378'\n",
      " 'Bid 13506 Facility 4.25.22'\n",
      " 'Heat Pump_Ages 18+, Homeowners in given counties'\n",
      " 'Requested Titles, Education Sector, Screening for K-12 Administrators '\n",
      " 'Consumer 2022 National Healthcare project'\n",
      " 'Ages 18+ within Comcast Footprint_Comcast Flex & SH'\n",
      " 'OR bid 14279 Packaging DMs '\n",
      " '5711_adults 18-64_(Pfizer); Comscore jobs 130757001-2'\n",
      " 'JACKSONVILLE & TALLAHASSEE Counties JAXT3205'\n",
      " 'USA Gen Pop in given markets'\n",
      " 'Follow Ups/Recontacts for Projects 10077742 & 10079439, IDs Provided (100%IR/5-10 Minutes)'\n",
      " 'OR bid 14302'\n",
      " 'Director+, Manufacturing and Technical Sector, 100+ EE size, Ages 20-64, USA (60%IR/15 Minutes)'\n",
      " '13-64 in India'\n",
      " \"Change Management  IS #4381 _FTE's/VP+ roles in all sectors\"\n",
      " 'Students in China' 'OR 12437' 'Group Insurance IN' 'Age 46-60 in Mexico'\n",
      " 'Tracker : Anti-Robocalling (Wave #4)'\n",
      " 'France, Germany, Spain - Architects, Builders, Facility Managers/Developers - Ref# 84291'\n",
      " 'Ages 18+ within Comcast Footprint_Comcast: AdPi Express Anthem'\n",
      " 'Students study'\n",
      " 'Orlando, Florida | Orlando Market-Format Study | Panel ID: ORLR3262 (Panel 1)'\n",
      " 'Ages 22-49 in Greater Toronto & Vancouver area_Renters - 22-0866'\n",
      " '22-0876 - Edmonton_Ages 18+ in given FSAâ€™s'\n",
      " 'Ages 18+ within Comcast Footprint_GSLXXXXXXKM: Comcast: Xfinity Watchathon'\n",
      " 'GSL100601LC: Abro Tracker: April Wave' 'Public Sentiment - April Wave'\n",
      " 'OMF Personal Loan LT Community Refresh' 'Pre/Post Brand Lift Study'\n",
      " 'Car owners/Auto shop DMs' 'US, 18-54, Retail Worker Study'\n",
      " 'DOI Community Recruit- 18+ gen pop ']\n"
     ]
    }
   ],
   "source": [
    "print(df.projects__name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14 entries, 0 to 13\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   rl                   14 non-null     int64 \n",
      " 1   projects__name       14 non-null     object\n",
      " 2   sentence             14 non-null     object\n",
      " 3   project_supplier_id  14 non-null     object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 576.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(subset=\"project_supplier_id\", keep=False, inplace=True, ignore_index=True)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rl                                     projects__name  \\\n",
      "0   565  Non African American male 35-54 in Clark count...   \n",
      "1   561                                    AZ354_PIRR_0422   \n",
      "2   445                                    AZ354_PIRR_0422   \n",
      "3   588                                    AZ354_PIRR_0422   \n",
      "4   341                                    AZ354_PIRR_0422   \n",
      "5   359                                    AZ354_PIRR_0422   \n",
      "6   565                       information workers - 147268   \n",
      "7   601                                  OR bid 14077 auto   \n",
      "8   588                                  OR bid 14077 auto   \n",
      "9   490                                  OR bid 14077 auto   \n",
      "10  537                 SWM211218 Community Sample Refresh   \n",
      "11  273                 Birth Control Tracker - April Wave   \n",
      "12  359  Suffolk Community College | LONR3216 | Long Is...   \n",
      "13  565                                     CX Pros & cons   \n",
      "\n",
      "                                             sentence  \\\n",
      "0                                  {'AGE': ['35-54']}   \n",
      "1                                  {'AGE': ['21-99']}   \n",
      "2                                  {'AGE': ['21-99']}   \n",
      "3                                  {'AGE': ['21-99']}   \n",
      "4                                  {'AGE': ['21-99']}   \n",
      "5                                  {'AGE': ['21-99']}   \n",
      "6                                  {'AGE': ['18-99']}   \n",
      "7                      {'GENDER': ['Female', 'Male']}   \n",
      "8                      {'GENDER': ['Female', 'Male']}   \n",
      "9                      {'GENDER': ['Female', 'Male']}   \n",
      "10  {'STANDARD_INDUSTRY_PERSONAL': ['Carpenting\\\\/...   \n",
      "11           {'GENDER': ['Female'], 'AGE': ['18-45']}   \n",
      "12  {'ZIP': ['11554 11710 11735 11736 11737 11756 ...   \n",
      "13                                 {'AGE': ['18-99']}   \n",
      "\n",
      "                                  project_supplier_id  \n",
      "0   Non African American male 35-54 in Clark count...  \n",
      "1                                 AZ354_PIRR_0422-561  \n",
      "2                                 AZ354_PIRR_0422-445  \n",
      "3                                 AZ354_PIRR_0422-588  \n",
      "4                                 AZ354_PIRR_0422-341  \n",
      "5                                 AZ354_PIRR_0422-359  \n",
      "6                    information workers - 147268-565  \n",
      "7                               OR bid 14077 auto-601  \n",
      "8                               OR bid 14077 auto-588  \n",
      "9                               OR bid 14077 auto-490  \n",
      "10             SWM211218 Community Sample Refresh-537  \n",
      "11             Birth Control Tracker - April Wave-273  \n",
      "12  Suffolk Community College | LONR3216 | Long Is...  \n",
      "13                                 CX Pros & cons-565  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "#STOPWORDS = set(stopwords.words('english'))\n",
    "MIN_WORDS = 4\n",
    "MAX_WORDS = 200\n",
    "'''\n",
    "PATTERN_S = re.compile(\"\\'s\")  # matches `'s` from text  \n",
    "PATTERN_RN = re.compile(\"\\\\r\\\\n\") #matches `\\r` and `\\n`\n",
    "PATTERN_PUNC = re.compile(r\"[^\\w\\s]\") # matches all non 0-9 A-z whitespace \n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Series of cleaning. String to lower case, remove non words characters and numbers.\n",
    "        text (str): input text\n",
    "    return (str): modified initial text\n",
    "    \"\"\"\n",
    "    text = text.lower()  # lowercase text\n",
    "    text = re.sub(PATTERN_S, ' ', text)\n",
    "    text = re.sub(PATTERN_RN, ' ', text)\n",
    "    text = re.sub(PATTERN_PUNC, ' ', text)\n",
    "    return text\n",
    "\n",
    "def tokenizer(sentence, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=False):\n",
    "    \"\"\"\n",
    "    Lemmatize, tokenize, crop and remove stop words.\n",
    "    \"\"\"\n",
    "    if lemmatize:\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        tokens = [stemmer.lemmatize(w) for w in word_tokenize(sentence)]\n",
    "    else:\n",
    "        tokens = [w for w in word_tokenize(sentence)]\n",
    "    token = [w for w in tokens if (len(w) > min_words and len(w) < max_words\n",
    "                                                        and w not in stopwords)]\n",
    "    return tokens    \n",
    "'''\n",
    "def tokenizer(sentence, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=None, lemmatize=False):\n",
    "    \"\"\"\n",
    "    Lemmatize, tokenize, crop and remove stop words.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = [w for w in word_tokenize(sentence)]\n",
    "    print(tokens)\n",
    "    '''\n",
    "    token = [w for w in tokens if (len(w) > min_words and len(w) < max_words\n",
    "                                                        and w not in stopwords)]\n",
    "    '''\n",
    "    return tokens\n",
    "\n",
    "def clean_sentences(df):\n",
    "    \"\"\"\n",
    "    Remove irrelavant characters (in new column clean_sentence).\n",
    "    Lemmatize, tokenize words into list of words (in new column tok_lem_sentence).\n",
    "    \"\"\"\n",
    "    print('Cleaning sentences...')\n",
    "    #df['clean_sentence'] = df['sentence'].apply(clean_text)\n",
    "    df['tok_lem_sentence'] = df['sentence'].apply(\n",
    "        lambda x: tokenizer(x, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=None, lemmatize=True))\n",
    "    return df\n",
    "    \n",
    "df = clean_sentences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sentence = \"STANDARD_JOB_TITLE': ['Owner, Partner, President', 'Director (Group Director, Sr. Director, Director)']\"\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_indices(m, topk, mask=None):\n",
    "    \"\"\"\n",
    "    Use sum of the cosine distance over all tokens.\n",
    "    m (np.array): cos matrix of shape (nb_in_tokens, nb_dict_tokens)\n",
    "    topk (int): number of indices to return (from high to lowest in order)\n",
    "    \"\"\"\n",
    "    # return the sum on all tokens of cosinus for each sentence\n",
    "    if len(m.shape) > 1:\n",
    "        cos_sim = np.mean(m, axis=0) \n",
    "    else: \n",
    "        cos_sim = m\n",
    "    index = np.argsort(cos_sim)[::-1] # from highest idx to smallest score \n",
    "    if mask is not None:\n",
    "        assert mask.shape == m.shape\n",
    "        mask = mask[index]\n",
    "    else:\n",
    "        mask = np.ones(len(cos_sim))\n",
    "    mask = np.logical_or(cos_sim[index] != 0, mask) #eliminate 0 cosine distance\n",
    "    best_index = index[mask][:topk]  \n",
    "    return best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Adapt stop words\n",
    "#token_stop = tokenizer(' '.join(STOPWORDS), lemmatize=False)\n",
    "\n",
    "# Fit TFIDF\n",
    "#vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer)\n",
    "vectorizer = TfidfVectorizer(stop_words=None, tokenizer=tokenizer) \n",
    "tfidf_mat = vectorizer.fit_transform(df['sentence'].values) # -> (num_sentences, num_vocabulary)\n",
    "tfidf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_tfidf(sentence, tfidf_mat):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the database sentences in order of highest cosine similarity relatively to each \n",
    "    token of the target sentence. \n",
    "    \"\"\"\n",
    "    # Embed the query sentence\n",
    "    tokens = [str(tok) for tok in tokenizer(sentence)]\n",
    "    vec = vectorizer.transform(tokens)\n",
    "    # Create list with similarity between query and dataset\n",
    "    mat = cosine_similarity(vec, tfidf_mat)\n",
    "    # Best cosine distance for each token independantly\n",
    "    print(mat.shape)\n",
    "    best_index = extract_best_indices(mat, topk=10)\n",
    "    return best_index\n",
    "\n",
    "\n",
    "best_index = get_recommendations_tfidf(query_sentence, tfidf_mat)\n",
    "\n",
    "display(df[['rl','projects__name','sentence']].iloc[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "#Load pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "# Apply the model to the sentences\n",
    "df['spacy_sentence'] = df['sentence'].apply(lambda x: nlp(x)) \n",
    "# Retrieve the embedded vectors as a matrix \n",
    "embed_mat = df['spacy_sentence'].values\n",
    "embed_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_spacy(model, query_sentence, embed_mat, topk=10):\n",
    "    \"\"\"\n",
    "    Predict the topk sentences after applying spacy model.\n",
    "    \"\"\"\n",
    "    query_embed = model(query_sentence)\n",
    "    mat = np.array([query_embed.similarity(line) for line in embed_mat])\n",
    "    # keep if vector has a norm\n",
    "    mat_mask = np.array(\n",
    "        [True if line.vector_norm else False for line in embed_mat])\n",
    "    best_index_2 = extract_best_indices(mat, topk=topk, mask=mat_mask)\n",
    "    return best_index_2\n",
    "\n",
    "# Predict\n",
    "best_index_2 = predict_spacy(nlp, query_sentence, embed_mat)\n",
    "display(df[['rl','projects__name','sentence']].iloc[best_index_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "corpus_embeddings = model.encode(df.sentence.values, convert_to_tensor=True)\n",
    "query_embedding = model.encode(query_sentence, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# We use cosine-similarity and torch.topk to find the highest 3 scores\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "top_results = torch.topk(cos_scores, k=10)\n",
    "\n",
    "print(\"\\n\\n======================\\n\\n\")\n",
    "print(\"Query:\", query_sentence)\n",
    "print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "for score, idx in zip(top_results[0], top_results[1]):\n",
    "    score = score.cpu().data.numpy() \n",
    "    idx = idx.cpu().data.numpy()\n",
    "    display(df[['rl','projects__name','sentence']].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BERT #####\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "BERT_BATCH_SIZE = 4\n",
    "MODEL_NAME = 'sentence-transformers/paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "class BertModel:\n",
    "    def __init__(self, model_name, device=-1, small_memory=True, batch_size=BERT_BATCH_SIZE):\n",
    "        self.model_name = model_name\n",
    "        self._set_device(device)\n",
    "        self.small_device = 'cpu' if small_memory else self.device\n",
    "        self.batch_size = batch_size\n",
    "        self.load_pretrained_model()\n",
    "\n",
    "    def _set_device(self, device):\n",
    "        if device == -1 or device == 'cpu':\n",
    "            self.device = 'cpu'\n",
    "        elif device == 'cuda' or device == 'gpu':\n",
    "            self.device = 'cuda'\n",
    "        elif isinstance(device, int) or isinstance(device, float):\n",
    "            self.device = 'cuda'\n",
    "        else:  # default\n",
    "            self.device = torch.device(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def load_pretrained_model(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        device = -1 if self.device == 'cpu' else 0\n",
    "        self.pipeline = pipeline('feature-extraction',\n",
    "                                 model=self.model, tokenizer=self.tokenizer, device=device)\n",
    "\n",
    "    def embed(self, data):\n",
    "        \"\"\" Create the embedded matrice from original sentences \"\"\"\n",
    "        nb_batchs = 1 if (len(data) < self.batch_size) else len(\n",
    "            data) // self.batch_size\n",
    "        batchs = np.array_split(data, nb_batchs)\n",
    "        mean_pooled = []\n",
    "        for batch in tqdm(batchs, total=len(batchs), desc='Training...'):\n",
    "            mean_pooled.append(self.transform(batch))\n",
    "        mean_pooled_tensor = torch.tensor(\n",
    "            len(data), dtype=float).to(self.small_device)\n",
    "        mean_pooled = torch.cat(mean_pooled, out=mean_pooled_tensor)\n",
    "        self.embed_mat = mean_pooled\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(\n",
    "            -1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def transform(self, data):\n",
    "        if 'str' in data.__class__.__name__:\n",
    "            data = [data]\n",
    "        data = list(data)\n",
    "        token_dict = self.tokenizer(\n",
    "            data,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\")\n",
    "        token_dict = self.to(token_dict, self.device)\n",
    "        with torch.no_grad():\n",
    "            token_embed = self.model(**token_dict)\n",
    "        # each of the 512 token has a 768 or 384-d vector depends on model)\n",
    "        attention_mask = token_dict['attention_mask']\n",
    "        # average pooling of masked embeddings\n",
    "        mean_pooled = self.mean_pooling(\n",
    "            token_embed, attention_mask)\n",
    "        mean_pooled = mean_pooled.to(self.small_device)\n",
    "        return mean_pooled\n",
    "    \n",
    "    def to(self, data: dict, device: str):\n",
    "        \"\"\"Send all values to device by calling v.to(device)\"\"\"\n",
    "        data = {k: v.to(device) for k, v in data.items()}\n",
    "        return data\n",
    "\n",
    "    def predict(self, in_sentence, topk=3):\n",
    "        input_vec = self.transform(in_sentence)\n",
    "        mat = cosine_similarity(input_vec, self.embed_mat)\n",
    "        # best cos sim for each token independantly\n",
    "        best_index = extract_best_indices(mat, topk=topk)\n",
    "        return best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU training\n",
    "bert_model = BertModel(model_name=MODEL_NAME, batch_size=BERT_BATCH_SIZE)\n",
    "bert_model.embed(df.sentence.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU training\n",
    "bert_model_gpu = BertModel(model_name=MODEL_NAME, batch_size=BERT_BATCH_SIZE, device='cuda')\n",
    "bert_model_gpu.transform(df.sentence.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_sentence = \"STANDARD_JOB_TITLE': ['Owner, Partner, President', 'Director (Group Director, Sr. Director, Director)']\"\n",
    "query_sentence = \"STANDARD_EMPLOYMENT\"\n",
    "indices = bert_model.predict(query_sentence, 10)\n",
    "#indices = bert_model_gpu.predict(query_sentence)\n",
    "display(df[['rl','projects__name','sentence']].iloc[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_sentence = \"STANDARD_JOB_TITLE': ['Owner, Partner, President', 'Director (Group Director, Sr. Director, Director)']\"\n",
    "query_sentence = \"STANDARD_EMPLOYMENT, STANDARD_INDUSTRY_PERSONAL\"\n",
    "indices = bert_model.predict(query_sentence, 10)\n",
    "#indices = bert_model_gpu.predict(query_sentence)\n",
    "display(df[['rl','projects__name','sentence']].iloc[indices])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
